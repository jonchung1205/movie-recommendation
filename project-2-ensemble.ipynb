{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the SVD ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from surprise import SVD, Reader, Dataset\n",
    "from surprise.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the folder containing the data\n",
    "data_folder = 'data_movie_lens_100k'\n",
    "\n",
    "# Load the additional data (user and movie info) from the specified folder\n",
    "user_info = pd.read_csv(os.path.join(data_folder, 'user_info.csv'))\n",
    "movie_info = pd.read_csv(os.path.join(data_folder, 'movie_info.csv'))\n",
    "\n",
    "# Example of encoding additional features for users and items\n",
    "# One-hot encode user info (age and gender)\n",
    "user_info['age_group'] = pd.cut(user_info['age'], bins=[0, 18, 30, 40, 50, 100], labels=[\"0-18\", \"19-30\", \"31-40\", \"41-50\", \"50+\"])\n",
    "\n",
    "# Use the correct argument for sparse matrix\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Encode the 'age_group' and 'is_male' columns separately\n",
    "encoded_user_info = encoder.fit_transform(user_info[['age_group', 'is_male']])\n",
    "\n",
    "# One-hot encode movie info (release year)\n",
    "movie_info['release_year'] = movie_info['release_year'].astype(str)\n",
    "encoder_movie = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Encode the 'release_year' column\n",
    "encoded_movie_info = encoder_movie.fit_transform(movie_info[['release_year']])\n",
    "\n",
    "# Now, assign the feature names correctly by using encoder.get_feature_names_out()\n",
    "user_info_encoded = pd.DataFrame(encoded_user_info, columns=encoder.get_feature_names_out(['age_group', 'is_male']))\n",
    "movie_info_encoded = pd.DataFrame(encoded_movie_info, columns=encoder_movie.get_feature_names_out(['release_year']))\n",
    "\n",
    "# Merge with the original datasets\n",
    "user_info = pd.concat([user_info, user_info_encoded], axis=1)\n",
    "movie_info = pd.concat([movie_info, movie_info_encoded], axis=1)\n",
    "\n",
    "# Merge user and movie info with the ratings data\n",
    "train_data = pd.read_csv(os.path.join(data_folder, \"ratings_all_development_set.csv\"))\n",
    "train_data = pd.merge(train_data, user_info, on=\"user_id\", how=\"left\")\n",
    "train_data = pd.merge(train_data, movie_info, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Convert to Surprise format\n",
    "from surprise import Reader, Dataset\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "train_dataset = Dataset.load_from_df(train_data[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# Train-test split for validation and testing\n",
    "from surprise.model_selection import train_test_split\n",
    "trainset, testset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150, 200],  # Try more factors\n",
    "    'reg_all': [0.1, 0.2, 0.3, 0.5],  # Try stronger regularization\n",
    "    'lr_all': [0.001, 0.002, 0.005],   # Lower learning rates\n",
    "    'n_epochs': [30, 50, 100]          # Try more epochs for training\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['mae'], cv=3)\n",
    "grid_search.fit(train_dataset)\n",
    "\n",
    "\n",
    "# Get the best model\n",
    "best_svd = grid_search.best_estimator['mae']\n",
    "\n",
    "# Train the best model on the entire training data\n",
    "trainset = train_dataset.build_full_trainset()\n",
    "best_svd.fit(trainset)\n",
    "\n",
    "# Test on the test set\n",
    "test_dataset = Dataset.load_from_df(train_data[['user_id', 'item_id', 'rating']], reader)  # Reuse train_data as example\n",
    "testset = test_dataset.build_full_trainset().build_testset()\n",
    "predictions = best_svd.test(testset)\n",
    "\n",
    "# Evaluate MAE\n",
    "from surprise import accuracy\n",
    "mae = accuracy.mae(predictions)\n",
    "print(f\"Test MAE: {mae}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the XGB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import autograd.numpy as ag_np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from CollabFilterOneVectorPerItem import CollabFilterOneVectorPerItem\n",
    "from train_valid_test_loader import load_train_valid_test_datasets\n",
    "\n",
    "DATA_DIR = './data_movie_lens_100k'\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "user_id_train, item_id_train, y_train = train_tuple\n",
    "user_id_valid, item_id_valid, y_valid = valid_tuple\n",
    "user_id_test, item_id_test, y_test = test_tuple\n",
    "\n",
    "train_df = pd.DataFrame({'user_id': user_id_train, 'item_id': item_id_train, 'rating': y_train})\n",
    "valid_df = pd.DataFrame({'user_id': user_id_valid, 'item_id': item_id_valid, 'rating': y_valid})\n",
    "test_df = pd.DataFrame({'user_id': user_id_test, 'item_id': item_id_test})\n",
    "\n",
    "user_means = train_df.groupby('user_id')['rating'].mean().rename(\"user_mean_rating\")\n",
    "item_means = train_df.groupby('item_id')['rating'].mean().rename(\"item_mean_rating\")\n",
    "\n",
    "train_df = train_df.merge(user_means, on='user_id').merge(item_means, on='item_id')\n",
    "valid_df = valid_df.merge(user_means, on='user_id', how='left').merge(item_means, on='item_id', how='left')\n",
    "test_df = test_df.merge(user_means, on='user_id', how='left').merge(item_means, on='item_id', how='left')\n",
    "\n",
    "valid_df.fillna({'user_mean_rating': train_df['rating'].mean(),\n",
    "                 'item_mean_rating': train_df['rating'].mean()}, inplace=True)\n",
    "test_df.fillna({'user_mean_rating': train_df['rating'].mean(),\n",
    "                'item_mean_rating': train_df['rating'].mean()}, inplace=True)\n",
    "\n",
    "X_train = train_df.drop(columns=[\"rating\"])\n",
    "y_train = train_df[\"rating\"]\n",
    "X_valid = valid_df.drop(columns=[\"rating\"])\n",
    "y_valid = valid_df[\"rating\"]\n",
    "X_test = test_df \n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# tuned hyperparameters\n",
    "params = {\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8\n",
    "}\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"eval\")]\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=1000, evals=evals, early_stopping_rounds=50)\n",
    "\n",
    "y_pred_test = xgb_model.predict(dtest)\n",
    "\n",
    "test_df[\"rating\"] = y_test \n",
    "test_mae = mean_absolute_error(test_df[\"rating\"], y_pred_test)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# leaderboard_df = pd.read_csv(\"data_movie_lens_100k/ratings_masked_leaderboard_set.csv\")\n",
    "\n",
    "# leaderboard_df = leaderboard_df.merge(user_means, on=\"user_id\", how=\"left\")\n",
    "# leaderboard_df = leaderboard_df.merge(item_means, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# leaderboard_df.fillna({\n",
    "#     \"user_mean_rating\": train_df[\"rating\"].mean(),\n",
    "#     \"item_mean_rating\": train_df[\"rating\"].mean()\n",
    "# }, inplace=True)\n",
    "\n",
    "# X_leaderboard = leaderboard_df.drop(columns=[\"rating\"])\n",
    "\n",
    "# dleaderboard = xgb.DMatrix(X_leaderboard)\n",
    "\n",
    "# y_pred_leaderboard = xgb_model.predict(dleaderboard)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the KNN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from surprise import Reader, Dataset, KNNBasic\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Specify the folder containing the data\n",
    "data_folder = 'data_movie_lens_100k'\n",
    "\n",
    "# Load the additional data (user and movie info) from the specified folder\n",
    "user_info = pd.read_csv(os.path.join(data_folder, 'user_info.csv'))\n",
    "movie_info = pd.read_csv(os.path.join(data_folder, 'movie_info.csv'))\n",
    "\n",
    "# Example of encoding additional features for users and items\n",
    "# One-hot encode user info (age and gender)\n",
    "user_info['age_group'] = pd.cut(user_info['age'], bins=[0, 18, 30, 40, 50, 100], labels=[\"0-18\", \"19-30\", \"31-40\", \"41-50\", \"50+\"])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Use the correct argument for sparse matrix\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Encode the 'age_group' and 'is_male' columns separately\n",
    "encoded_user_info = encoder.fit_transform(user_info[['age_group', 'is_male']])\n",
    "\n",
    "# One-hot encode movie info (release year)\n",
    "movie_info['release_year'] = movie_info['release_year'].astype(str)\n",
    "encoder_movie = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Encode the 'release_year' column\n",
    "encoded_movie_info = encoder_movie.fit_transform(movie_info[['release_year']])\n",
    "\n",
    "# Now, assign the feature names correctly by using encoder.get_feature_names_out()\n",
    "user_info_encoded = pd.DataFrame(encoded_user_info, columns=encoder.get_feature_names_out(['age_group', 'is_male']))\n",
    "movie_info_encoded = pd.DataFrame(encoded_movie_info, columns=encoder_movie.get_feature_names_out(['release_year']))\n",
    "\n",
    "# Merge with the original datasets\n",
    "user_info = pd.concat([user_info, user_info_encoded], axis=1)\n",
    "movie_info = pd.concat([movie_info, movie_info_encoded], axis=1)\n",
    "\n",
    "# Load and merge the ratings data\n",
    "train_data = pd.read_csv(os.path.join(data_folder, \"ratings_all_development_set.csv\"))\n",
    "train_data = pd.merge(train_data, user_info, on=\"user_id\", how=\"left\")\n",
    "train_data = pd.merge(train_data, movie_info, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Convert to Surprise format\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "surprise_data = Dataset.load_from_df(train_data[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'k': [20, 30, 50, 100],  # Number of neighbors\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson', 'msd'],\n",
    "        'user_based': [True, False]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best parameters\n",
    "gs = GridSearchCV(KNNBasic, param_grid, measures=['mae'], cv=5, n_jobs=-1)\n",
    "gs.fit(surprise_data)\n",
    "\n",
    "# Output the best score and parameters\n",
    "print(f\"Best MAE: {gs.best_score['mae']}\")\n",
    "print(f\"Best Parameters: {gs.best_params['mae']}\")\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model = gs.best_estimator['mae']\n",
    "trainset = surprise_data.build_full_trainset()\n",
    "best_model.fit(trainset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Average Ensemble ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, accuracy\n",
    "from surprise.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data (assuming the paths are correct)\n",
    "data_folder = 'data_movie_lens_100k'\n",
    "user_info = pd.read_csv(os.path.join(data_folder, 'user_info.csv'))\n",
    "movie_info = pd.read_csv(os.path.join(data_folder, 'movie_info.csv'))\n",
    "train_data = pd.read_csv(os.path.join(data_folder, \"ratings_all_development_set.csv\"))\n",
    "\n",
    "# Example of encoding user and movie info\n",
    "user_info['age_group'] = pd.cut(user_info['age'], bins=[0, 18, 30, 40, 50, 100], labels=[\"0-18\", \"19-30\", \"31-40\", \"41-50\", \"50+\"])\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_user_info = encoder.fit_transform(user_info[['age_group', 'is_male']])\n",
    "\n",
    "movie_info['release_year'] = movie_info['release_year'].astype(str)\n",
    "encoder_movie = OneHotEncoder(sparse_output=False)\n",
    "encoded_movie_info = encoder_movie.fit_transform(movie_info[['release_year']])\n",
    "\n",
    "user_info_encoded = pd.DataFrame(encoded_user_info, columns=encoder.get_feature_names_out(['age_group', 'is_male']))\n",
    "movie_info_encoded = pd.DataFrame(encoded_movie_info, columns=encoder_movie.get_feature_names_out(['release_year']))\n",
    "\n",
    "user_info = pd.concat([user_info, user_info_encoded], axis=1)\n",
    "movie_info = pd.concat([movie_info, movie_info_encoded], axis=1)\n",
    "\n",
    "# Merge with ratings\n",
    "train_data = pd.merge(train_data, user_info, on=\"user_id\", how=\"left\")\n",
    "train_data = pd.merge(train_data, movie_info, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Assuming you have already split user_id_test, item_id_test, and ratings for the test set\n",
    "test_data = pd.DataFrame({'user_id': user_id_test, 'item_id': item_id_test, 'rating': y_test})\n",
    "test_data = pd.merge(test_data, user_info, on=\"user_id\", how=\"left\")\n",
    "test_data = pd.merge(test_data, movie_info, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Prepare the feature matrix X_test for XGBoost and KNN\n",
    "X_test = test_data.drop(columns=['user_id', 'item_id', 'rating'])  # remove target column\n",
    "\n",
    "# Convert categorical columns to numeric using pd.get_dummies\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Ensure the columns match between the training and test sets\n",
    "X_train = train_data.drop(columns=['user_id', 'item_id', 'rating'])  # remove target column\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# Remove duplicate columns from X_train and X_test\n",
    "X_train = X_train.loc[:, ~X_train.columns.duplicated()]\n",
    "X_test = X_test.loc[:, ~X_test.columns.duplicated()]\n",
    "\n",
    "# Align columns between training and test data\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array to avoid dtype issues with DMatrix\n",
    "X_test_np = X_test.values\n",
    "X_train_np = X_train.values\n",
    "\n",
    "# For XGBoost, ensure you convert it to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train_np, label=train_data['rating'].values)\n",
    "dtest = xgb.DMatrix(X_test_np)  # X_test should be in the same format as the training data\n",
    "\n",
    "# Train the XGBoost model\n",
    "params = {\n",
    "    \"objective\": \"reg:absoluteerror\",\n",
    "    \"eval_metric\": \"mae\",\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8\n",
    "}\n",
    "evals = [(dtrain, \"train\")]\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=1000, evals=evals, early_stopping_rounds=50)\n",
    "\n",
    "# Get predictions from XGBoost model\n",
    "xgb_pred = xgb_model.predict(dtest)\n",
    "\n",
    "# Assuming KNN and SVD models are already trained and predictions are available\n",
    "knn_pred = []\n",
    "for _, row in test_data.iterrows():\n",
    "    knn_pred.append(best_model.predict(row['user_id'], row['item_id']).est)\n",
    "knn_pred = np.array(knn_pred)\n",
    "\n",
    "svd_pred = []\n",
    "for _, row in test_data.iterrows():\n",
    "    svd_pred.append(best_svd.predict(row['user_id'], row['item_id']).est)\n",
    "svd_pred = np.array(svd_pred)\n",
    "\n",
    "# Assign weights (these can be based on model performance, e.g., validation MAE)\n",
    "w_xgb = 0.5  # weight for xgb_model\n",
    "w_knn = 0.3  # weight for best_model (KNN)\n",
    "w_svd = 0.2  # weight for best_svd\n",
    "\n",
    "# Compute the weighted average\n",
    "ensemble_pred = (w_xgb * xgb_pred + w_knn * knn_pred + w_svd * svd_pred) / (w_xgb + w_knn + w_svd)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "test_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "print(f\"Ensemble MAE: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Model Ensemble ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate predictions on the training set\n",
    "xgb_pred_train = xgb_model.predict(dtrain)\n",
    "knn_pred_train = np.array([best_model.predict(uid, iid).est for uid, iid in zip(train_data['user_id'], train_data['item_id'])])\n",
    "svd_pred_train = np.array([best_svd.predict(uid, iid).est for uid, iid in zip(train_data['user_id'], train_data['item_id'])])\n",
    "\n",
    "# Create meta-features for the training set\n",
    "meta_features_train = np.column_stack((xgb_pred_train, knn_pred_train, svd_pred_train))\n",
    "meta_target_train = train_data['rating'].values\n",
    "\n",
    "# Train the meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(meta_features_train, meta_target_train)\n",
    "\n",
    "# Generate predictions on the test set\n",
    "xgb_pred_test = xgb_model.predict(dtest)\n",
    "knn_pred_test = np.array([best_model.predict(uid, iid).est for uid, iid in zip(test_data['user_id'], test_data['item_id'])])\n",
    "svd_pred_test = np.array([best_svd.predict(uid, iid).est for uid, iid in zip(test_data['user_id'], test_data['item_id'])])\n",
    "\n",
    "# Create meta-features for the test set\n",
    "meta_features_test = np.column_stack((xgb_pred_test, knn_pred_test, svd_pred_test))\n",
    "\n",
    "# Predict using the meta-model\n",
    "ensemble_pred = meta_model.predict(meta_features_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "test_mae = mean_absolute_error(y_test, ensemble_pred)\n",
    "print(f\"Ensemble MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Load the leaderboard dataset (ratings_masked_leaderboard_set.csv)\n",
    "leaderboard_set = pd.read_csv(os.path.join(data_folder, 'ratings_masked_leaderboard_set.csv'))\n",
    "leaderboard_set = pd.merge(leaderboard_set, user_info, on=\"user_id\", how=\"left\")\n",
    "leaderboard_set = pd.merge(leaderboard_set, movie_info, on=\"item_id\", how=\"left\")\n",
    "\n",
    "# Prepare the feature matrix X_leaderboard for XGBoost and KNN\n",
    "X_leaderboard = leaderboard_set.drop(columns=['user_id', 'item_id', 'rating'])  # remove target column\n",
    "X_leaderboard = pd.get_dummies(X_leaderboard, drop_first=True)\n",
    "\n",
    "# Remove duplicate columns from X_leaderboard\n",
    "X_leaderboard = X_leaderboard.loc[:, ~X_leaderboard.columns.duplicated()]\n",
    "\n",
    "# Ensure the columns match between the training and leaderboard sets\n",
    "X_leaderboard = X_leaderboard.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Convert the DataFrame to a NumPy array to avoid dtype issues with DMatrix\n",
    "X_leaderboard_np = X_leaderboard.values\n",
    "dleaderboard = xgb.DMatrix(X_leaderboard_np)\n",
    "\n",
    "# Get predictions from each pre-trained model\n",
    "xgb_pred_leaderboard = xgb_model.predict(dleaderboard)\n",
    "knn_pred_leaderboard = np.array([best_model.predict(uid, iid).est for uid, iid in zip(leaderboard_set['user_id'], leaderboard_set['item_id'])])\n",
    "svd_pred_leaderboard = np.array([best_svd.predict(uid, iid).est for uid, iid in zip(leaderboard_set['user_id'], leaderboard_set['item_id'])])\n",
    "\n",
    "# Create meta-features for the leaderboard set\n",
    "meta_features_leaderboard = np.column_stack((xgb_pred_leaderboard, knn_pred_leaderboard, svd_pred_leaderboard))\n",
    "\n",
    "# Predict using the meta-model\n",
    "ensemble_pred_leaderboard = meta_model.predict(meta_features_leaderboard)\n",
    "\n",
    "# Save the ensemble predictions to a plain text file\n",
    "np.savetxt('predicted_ratings_ensemble_leaderboard.txt', ensemble_pred_leaderboard, fmt='%f')\n",
    "\n",
    "# Confirm the format is correct\n",
    "print(\"Predictions saved as 'predicted_ratings_ensemble_leaderboard.txt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
